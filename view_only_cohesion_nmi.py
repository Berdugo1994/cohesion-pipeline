# -*- coding: utf-8 -*-
"""View Only - Cohesion NMI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RreFOEd5LQDaNB7kQcH44bk5gkTpSVeH

## **Cohesion POC** - Topic Detection Measurement
 


## Cohesion
The Topic-Detection field deals mainly with providing names to given divisions of documents and lacks a quality measurement that provides a rating for the division, that represent a human-subjective score.
<br>
<br>
Cohesion is here to overcome it, includes NLP techniques and considers intra and inter scores in the cohesion formula.
<br>
<br>
The goal of the POC, it to prove that Cohesion has better correlation with NMI heuristic than Coherence which consider as SOA of Topic-Detecion-Measurement domain.
<br>

![image.png](data:image/png;base64,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)

## Environment configuration

Install and import the required packages and resources.
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U sentence-transformers
!pip install tqdm
!pip install termcolor

"""## Imports"""

import re
import json
import copy
import random

import numpy as np
import pandas as pd

import time
from tabulate import tabulate
import warnings
warnings.filterwarnings('ignore')
from tqdm import tqdm
from termcolor import colored

from transformers import pipeline, PreTrainedTokenizerFast
from gensim.corpora import Dictionary
from gensim.models import CoherenceModel
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.cluster import normalized_mutual_info_score

from scipy import stats
import matplotlib.pyplot as plt
from collections import Counter
import statistics
from scipy.spatial.distance import euclidean

import nltk
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
stop_words = set(stopwords.words('english'))

"""##Run Configuration"""

use_GloVe = False
use_synonyms = False
random.seed(10)

chosen_data = "Events2012"
# chosen_data = "20news"
# chosen_data = "Handmade"

if chosen_data=="Events2012":
  topics_range = range(17,300,7)
  docs_precent = 0.3 # % of "selected_docs" to test against "selected topics"
  topics_precent = 0.15 # % of "selected topics" to test against "selected_docs"
  POC_range = range(0,101,10)
  tf_idf_common_threshold = 0.4

elif chosen_data== "20news":
  topics_range = (8,12)
  docs_precent = 0.8 # % of "selected_docs" to test against "selected topics"
  topics_precent = 1/3 # % of "selected topics" to test against "selected_docs"
  POC_range = range(0,101,50)
  tf_idf_common_threshold = 0.33

elif chosen_data== "Handmade":
  topics_range = range(1,6,1)
  docs_precent = 1 # % of "selected_docs" to test against "selected topics"
  topics_precent = 1 # % of "selected topics" to test against "selected_docs"
  POC_range=range(0,100,35)
  tf_idf_common_threshold = 0.33

"""##Loading dataset

Choosing desired dataset
"""

if chosen_data=="Events2012":
  original_df = pd.read_csv('/content/drive/Shareddrives/Mezada/Cohesion/Datasets/event2012.tsv', sep='\t', usecols=['label', 'text'])

if chosen_data == "20news":
  from sklearn.datasets import fetch_20newsgroups
  groups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))
  original_df = pd.DataFrame({"text":groups.data,"label":groups.target})

if chosen_data == "Handmade":
  original_df = pd.read_csv('/content/drive/Shareddrives/Mezada/Cohesion/Datasets/HandMade/good_division.txt', sep='\t', usecols=['label', 'text'])

#selected here means - selected by configuration.
# filter df by selected topics
ground_truth_df = original_df[original_df['label'].apply(lambda label: label in topics_range)] # df of filtered

# selected documents as one list
ground_truth_docs_list = ground_truth_df['text'].tolist()

# selected labels as one list
ground_truth_label_list = ground_truth_df['label'].tolist()

"""## Dataset plots"""

ground_truth_df.head(5)

all_docs = original_df['text'].tolist()
docs_length_list = [len(doc.split()) for doc in all_docs]
plt.title("Words length distribution")
plt.hist(docs_length_list, density=True, bins=10) 
mean = statistics.mean(docs_length_list)
print("Mean is {:.2f}".format(mean))

word_frequency = Counter(" ".join(all_docs).split()).most_common(10)
words = [word for word, _ in word_frequency]
counts = [counts for _, counts in word_frequency]
plt.bar(words, counts)
plt.title("10 most frequent tokens in corpus")
plt.ylabel("Frequency")
plt.xlabel("Words")
plt.show()

"""As we can see, Preproccess - clean stopwords is a mandatory stage, specially in our dataset!

##Utils
"""

def tokenizer(text):
    word_tokens = word_tokenize(text)
    bad_words_list = ["http"]
    filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words and w.isalpha() and len(w)>2 and w not in bad_words_list]
    return filtered_sentence

def docs_to_groups(docs, clusters):
    clusters_docs = dict()
    for index,doc in enumerate(docs):
        cluster = int(clusters[index])
        docs_list = clusters_docs.get(cluster,list())
        docs_list.append(doc)
        clusters_docs[cluster] = docs_list
    return clusters_docs

def create_input_to_mnli_topics_content(content_dict, path):
    content_list = []
    topics_ind_list = []
    for topic_ind in sorted(content_dict.keys()):
        content_list += content_dict[topic_ind]
        topics_ind_list += [topic_ind] * len(content_dict[topic_ind])
    df_topic_id_content = pd.DataFrame({"topic_ind": topics_ind_list, "content": content_list})
    df_topic_id_content.to_pickle(path, protocol=4)

def create_topic_id_topic_value(topics_ind_list, topic_values, path):
    topic_values_joined = []
    for ten_words_seperated in topic_values:
        topic_values_joined.append(" ".join(ten_words_seperated))
    df_topic_id_value_coherence = pd.DataFrame({"topic_ind": topics_ind_list, "topic_name": topic_values_joined})
    df_topic_id_value_coherence.to_pickle(path, protocol=4)

def tf_idf_top_words(docs: list, words_count=6):
    vectorizer = TfidfVectorizer(use_idf=True, stop_words='english', ngram_range=(1, 1), tokenizer=tokenizer)
    vectors = vectorizer.fit_transform(docs)
    feature_names = vectorizer.get_feature_names()
    dense = vectors.todense()
    denselist = dense.tolist()
    df = pd.DataFrame(denselist, columns=feature_names)
    return list(df.mean(axis=0).sort_values(ascending=False).index)[0:words_count]

def enhance_topic_with_synonyms(word):
    word = word.lower()
    synonyms = []
    synsets = wordnet.synsets(word)
    if (len(synsets) == 0):
        return []
    synset = synsets[0]
    lemma_names = synset.lemma_names()
    for lemma_name in lemma_names:
        lemma_name = lemma_name.lower().replace('_', ' ')
        if (lemma_name != word and lemma_name not in synonyms):
            synonyms.append(lemma_name)
    return synonyms[:5]

def docs_to_clusters(docs_list, docs_label):
    clusters_docs = dict()
    for index, label in enumerate(docs_label):
        doc_list = clusters_docs.get(label, list())
        doc_list.append(docs_list[index])
        clusters_docs[label] = doc_list
    return clusters_docs

def get_division(docs_label):
    clusters_docs = dict()
    for index, label in enumerate(docs_label):
        index_list = clusters_docs.get(label, list())
        index_list.append(index)
        clusters_docs[label] = index_list
    return clusters_docs

def create_topic_name(key, labels_array):
    topic_name = str(key)
    for label in labels_array:
      topic_name += '_' + label
    return topic_name

def create_topic_names_using_tf_idf(docs, labels):
    tf_idf_labels, tf_idf_labels_synonyms, tf_idf_labels_glove  = dict(), dict(), dict()
    docs_groups = docs_to_clusters(docs, labels)
    for key, value in docs_groups.items():
      labels_array = tf_idf_top_words(value, words_count=5)
      tf_idf_labels[key] = create_topic_name(key, labels_array)

      if use_synonyms:
          labels_array_synonyms = synonyms_enhance(copy.deepcopy(labels_array))
          tf_idf_labels_synonyms[key] = create_topic_name(key, labels_array_synonyms)

      if use_GloVe:
          labels_array_for_glove = copy.deepcopy(labels_array)
          remove_nonsimilar_from_topic(labels_array_for_glove)
          tf_idf_labels_glove[key] = create_topic_name(key, labels_array_for_glove)

    return tf_idf_labels, tf_idf_labels_synonyms, tf_idf_labels_glove

def create_topic_names_using_tf_idf_with_optimization(docs, labels):
    # Optional: alternative function to 'create_topic_names_using_tf_idf'
    # Use this func to optimize TF-IDF selected words between topics
    porter = PorterStemmer()
    tf_idf_labels, tf_idf_labels_synonyms, tf_idf_labels_glove, topics_commonly, all_topics_words  = dict(), dict(), dict(), dict(), dict()
    docs_groups = docs_to_clusters(docs, labels)
    for key, value in docs_groups.items():
        labels_array = tf_idf_top_words(value, words_count=7)
        all_topics_words[key] = labels_array
        for word in labels_array:
            stemmed_word = porter.stem(word)
            count = topics_commonly.get(stemmed_word,0)
            topics_commonly[stemmed_word] = count +1
    optimize_topic_words(all_topics_words, topics_commonly, tf_idf_labels, docs_groups, words_count=7,final_words_count=3)
    return tf_idf_labels, tf_idf_labels_synonyms, tf_idf_labels_glove


def optimize_topic_words(all_topics_words, topics_commonly, tf_idf_labels, docs_groups, words_count=7,final_words_count=5):
    porter = PorterStemmer()
    topics_amount = len(docs_groups.keys())
    for key, value in docs_groups.items():
        final_words = [""] * final_words_count
        replaced_count = 0
        for i in range(final_words_count-1,-1,-1):
            stemmed_word = porter.stem(all_topics_words[key][i])
            if topics_commonly[stemmed_word]/topics_amount > tf_idf_common_threshold and replaced_count<words_count-final_words_count:
              alternative_word = all_topics_words[key][final_words_count+replaced_count]
              alternative_word_stemmed = porter.stem(alternative_word)
              if topics_commonly[stemmed_word]>topics_commonly[alternative_word_stemmed]:
                  print("stemmed word {} - replaced with {}".format(stemmed_word,alternative_word))
                  final_words[i] = alternative_word
                  replaced_count+=1
              else:
                # The word is common but the alternative is even worst.
                  final_words[i]=all_topics_words[key][i]
              # IF we would like to extract this word from *some* of the topics, it should be done here.
              # topics_commonly[stemmed_word] = topics_commonly[stemmed_word] - 1
            else:
              final_words[i]=all_topics_words[key][i]
        tf_idf_labels[key] = create_topic_name(key, final_words)

def create_random_divisions(ground_truth_df, POC_range):
    division_dict = dict()
    for random_percent in POC_range:
      randomized_labels = create_division(ground_truth_df, random_percent=random_percent)
      division_dict[random_percent] = randomized_labels
    return division_dict 

def create_division(ground_truth_df, random_percent):
    # Helper function for create_random_divisions.
    # Creates a division given ground truth division and random-error-percent
    labels = ground_truth_df["label"].to_list()
    result = labels.copy() # [0,0,1,1,2,2]
    labels_indexes = range (len(result)) # [0,1,2,3,4,5]
    labels_set = sorted(set(labels)) # [0,1,2]
    indexes_to_be_replaced = random.sample(labels_indexes, int((random_percent / 100)*len(labels_indexes)))
    for index_to_be_replace in indexes_to_be_replaced:
      alternative_labels = list(labels_set)
      alternative_labels.remove(result[index_to_be_replace])
      result[index_to_be_replace] = random.sample(alternative_labels, 1)[0]
    return result

def extract_topics_labels(topics_names_dict):
    topics_names = []
    for topic_ind in sorted(topics_names_dict.keys()):
      topics_names.append(topics_names_dict[topic_ind].split('_')[1:])
    return topics_names

def models_to_run_over(topics, topics_synonyms, topics_glove):
    models_run_over = [topics]
    if use_synonyms:
        models_run_over.append(topics_synonyms)
    if use_GloVe:
        models_run_over.append(topics_glove)
    return models_run_over

def get_titles():
    titles = ['Regular']
    if use_synonyms:
        titles.append('Synonyms')
    if use_GloVe:
        titles.append('GloVe')
    return titles

# ----------- Prints -----------
      
def summary_print():
    print("Data: " + chosen_data)
    print('There are {} docs in the dataset'.format(len(ground_truth_label_list)))
    print('Topics range is {}'.format(topics_range))
    print('Docs {:.1f}% Tested'.format(docs_precent*100))
    print('Topics (Negative %) {:.1f}% Tested'.format(topics_precent*100))
    for i in range(len(get_titles())):
        # present results
        print('\n---------------------- Results {} -------------------------'.format(get_titles()[i]))
        print(tabulate(total_for_print[i], headers=['error %','nmi', 'coherence', 'cohesion'], tablefmt='orgtbl'))

def detailed_division_print(division_dict):
    print("\n","*"*5, "Randomized Divisions", "*"*5)
    for random_percent in POC_range:
        randomized_labels = division_dict[random_percent]
        real_change = 0
        for t,k in enumerate(randomized_labels):
            if k != division_dict[0][t]:
                real_change += 1
        print("random precent:", str(random_percent) + "%", "real change: {:.2f}".format(real_change/len(randomized_labels)), " labels:",randomized_labels)

def detailed_topics_print(topics_names):
    print("\n","*"*5, "\tTopics\t", "*"*5)
    for i,topic_name in enumerate(topics_names):
        print("Topic {} value is {}".format(i, topic_name))

def print_title(index):
    print("\n", "_"*30, "\nRun Mode:",colored(get_titles()[index], 'red'))

def print_results(nmi_score, coherence_score, cohesion_score):
        print('nmi_score: ' + str(nmi_score))
        print('coherence_score: ' + str(coherence_score))
        print('cohesion_score: ' + str(cohesion_score))

"""### GloVe Utils"""

## GLOVE
emmbed_glove_dict = {}
if use_GloVe:
    with open('/content/drive/Shareddrives/Mezada/Cohesion/Datasets/GloVe/glove.twitter.27B.25d.txt','r') as f:
      for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:],'float32')
        emmbed_glove_dict[word]=vector

def get_sum_of_similarity(word_to_compare,words):
    """
      GloVe logic - sums the distance between a chosen word from a topic to all the other words in the topic
      * if a word not exist in GloVe dictionary, it will get Perfect(0) distance so it will be not erased from topic names,
        The reason behind it-words that not exist in GloVe model are unique to collection.
    """
    if len(emmbed_glove_dict.get(word_to_compare, [])) == 0:
      return 999
    dist = 0
    words = words.copy()
    words.remove(word_to_compare)
    for w in words:
      dist += 0 if len(emmbed_glove_dict.get(w, [])) == 0 else euclidean(emmbed_glove_dict[word_to_compare], emmbed_glove_dict[w])
    return dist

def remove_nonsimilar_from_topic(topic):
    sums = [get_sum_of_similarity(w, topic) for w in topic]
    topic.pop(sums.index(max(sums)))

"""###WordNet Utils"""

if use_synonyms:
  all_words = set()
  for doc in ground_truth_docs_list:
    all_words = all_words.union(set(tokenizer(doc)))

def synonyms_enhance(labels_array):
    words_to_add = []
    for w in labels_array:
      synonyms_array = enhance_topic_with_synonyms(w)
      for syn in synonyms_array:
        syn = syn.split(' ')
        words_to_add += [single_word for single_word in syn if single_word in all_words]
    labels_array += words_to_add
    return list(set(labels_array))

def enhance_topic_with_synonyms(word, words_to_add_for_each_words=3):
    word = word.lower()
    synonyms = []
    synsets = wordnet.synsets(word)
    if (len(synsets) == 0):
        return []
    synset = synsets[0]
    lemma_names = synset.lemma_names()
    for lemma_name in lemma_names:
        lemma_name = lemma_name.lower().replace('_', ' ')
        if (lemma_name != word and lemma_name not in synonyms):
            synonyms.append(lemma_name)
    return synonyms[:words_to_add_for_each_words]

"""## Zero-Shot classifier"""

class ZeroShotClassifier:
    def __init__(self, path_dic_topic_id_to_name, path_df_docs_topics, topics):
        self.model_name = "facebook/bart-large-mnli"
        self.model_name_saveable = "bart-large-mnli"
        self.df_docs_topics = pd.read_pickle(path_df_docs_topics)
        self.dic_topic_id_to_name = self.create_topic_ind_name_from_pkl(path_dic_topic_id_to_name)
        self.classifier = pipeline("zero-shot-classification",
                          model=self.model_name, device=0) #  remove device=0 in case you don't use GPU.
        self.docs, self.series_topics = self.get_topics_and_docs_filtered()

    def run_classify(self, division_labels, path_to_save):
        """
          Cohesion "Heart" - Uses MNLI to find similarity between docs and topics.
          results saved into 'path_to_save'
        """
        topic_names=[]
        for i in sorted(self.dic_topic_id_to_name.keys()):
          topic_names.append(self.dic_topic_id_to_name[i])
        division_dict = get_division(division_labels)
        doc_values_to_test, docs_ind_of_selected_docs, topics_ind_of_selected_docs = self.get_selected_docs(division_dict)
        results = self.zero_shot_to_every_doc_and_random_topics(doc_values_to_test, topic_names, topics_ind_of_selected_docs)
        df = self.create_df_zero_shot_results(topic_names, results, docs_ind_of_selected_docs)
        self.save_mnli_scores(df, path_to_save)
        

    def get_topics_and_docs_filtered(self):
        """
        drops docs that their topic id is not defined in dic_topic_id_to_name
        series topics is topic name for every doc that pass the filter
        returns docs and topics
        """
        series_topics = self.df_docs_topics["topic_ind"].apply(lambda x: self.dic_topic_id_to_name.get(x, np.nan)).dropna()
        docs = self.df_docs_topics["content"][series_topics.index].to_list()
        return docs, series_topics

    def get_selected_topics(self, topic_index_of_doc, topics, set_indexes_real_topics, topics_dict):
        selected_topics = []
        optional_topics = sorted(set_indexes_real_topics)
        optional_topics.remove(topic_index_of_doc)
        amount_of_topics_to_test = 1 + int(len(optional_topics) * topics_precent) # % of relevant topics + 1 (to ensure negative topic as well) + 1 related topic! *MINIMUM amount of topics is 3(!)
        amount_of_topics_to_test = min(amount_of_topics_to_test,len(optional_topics))
        selected_topics_ind = list(random.sample(optional_topics, amount_of_topics_to_test))
        selected_topics_ind = [topic_index_of_doc] + selected_topics_ind # related label is always at [0].
        for top_ind in selected_topics_ind:
            selected_topics.append(topics_dict[top_ind].split())
        return selected_topics

    def zero_shot_to_every_doc_and_random_topics(self, docs, topics, indexes_real_topic):
        topics_dict=dict()
        sorted_set_indexes_real_topic = sorted(set(indexes_real_topic))
        for i,top_ind in enumerate(sorted_set_indexes_real_topic):
          topics_dict[top_ind] = topics[i]
        results = []
        print_single = True
        set_indexes_real_topics = set(indexes_real_topic)
        for i, doc in enumerate(docs):
            topic_index_of_doc = indexes_real_topic[i]
            selected_topics = self.get_selected_topics(topic_index_of_doc, topics, set_indexes_real_topics, topics_dict)
            doc_topics_result = self.classifier(doc, selected_topics, multi_label=True)
            doc_topics_result["labels"] = [" ".join(label) for label in doc_topics_result["labels"]]
            results.append(doc_topics_result)
        return results

    def save_mnli_scores(self, df, path):
        df.to_pickle(path + ".pkl", protocol=4)
        df.to_csv(path + ".csv")

    def create_topic_ind_name_from_pkl(self, path):
        df_temp = pd.read_pickle(path)
        df_temp = df_temp[df_temp['topic_ind'] != -1]  # drops unassigned docs
        return dict(zip(df_temp["topic_ind"].to_list(), df_temp["topic_name"].to_list()))
    
    def get_selected_docs(self, division_dict):
        topics_ind_of_selected_docs = []
        docs_ind_of_selected_docs = []
        for i in division_dict.keys():
            docs_in_topic = division_dict.get(i)
            related_docs_amount = int(len(docs_in_topic) * docs_precent)
            related_docs_amount = related_docs_amount if related_docs_amount>2 else min(2,len(docs_in_topic))
            try:
              selected_docs_from_this_topic = random.sample(docs_in_topic, related_docs_amount)
            except:
              print( related_docs_amount - 1)
              print(docs_in_topic)
            docs_ind_of_selected_docs += selected_docs_from_this_topic
            topics_ind_of_selected_docs += [i] * len(selected_docs_from_this_topic)
        doc_values_to_test = []
        for ind in docs_ind_of_selected_docs:
            doc_values_to_test.append(ground_truth_docs_list[ind])
        return doc_values_to_test, docs_ind_of_selected_docs, topics_ind_of_selected_docs

    def create_df_zero_shot_results(self, topic_names, results, docs_ind_of_selected_docs):
        topics_by_doc = self.series_topics.to_list() # list of topics values, for every doc
        topic_names_with_ind = [str(i)+"_"+topic_name for i,topic_name in enumerate(topic_names)]
        df = pd.DataFrame(columns=["doc_num", "doc", "real_topic"] + topic_names_with_ind)
        for i, result in enumerate(results):
            doc_ind = docs_ind_of_selected_docs[i]
            doc_labels, doc_scores , doc_value = result["labels"], result["scores"], result["sequence"]
            doc_dict = {"doc_num": doc_ind, "doc": doc_value, "real_topic": topics_by_doc[doc_ind]}
            for topic_i, spec_topic in enumerate(self.dic_topic_id_to_name.values()):
                doc_topic_score = 0 # default value for doc - topic. there is no mandatory to test each topic against each doc.
                if spec_topic in doc_labels:
                    index_of_spec_topic = doc_labels.index(spec_topic)
                    doc_topic_score = doc_scores[index_of_spec_topic]
                doc_dict[str(topic_i)+"_"+spec_topic] = doc_topic_score
            df = df.append(doc_dict, ignore_index=True)
        return df

"""## Cohesion Formula"""

def cohesion_calculate(path):
    docs_col, topic_col = generate_docs_topics_collections(path)
    topics_with_scores = calculate_topics_score(docs_col, topic_col)
    score = calculate_topics_cohesion_ratio(topics_with_scores)
    return score

def generate_docs_topics_collections(path):
    data, topics = read_zero_shot_results(path)
    docs_collection = insert_data_to_docs_collection(data, topics)
    topics_collection = insert_data_to_topics_collection(topics)
    return docs_collection, topics_collection

def read_zero_shot_results(path):
    authorized_columns = ['Unnamed: 0', 'doc', 'real_topic']
    data = pd.read_csv(path)
    topics = data.columns
    topics_without_underscore = []
    for topic in topics:
        if topic in authorized_columns:
            topics_without_underscore.append(topic)
        elif topic == 'doc_num':
            continue
        else:
            split_topic = topic.split('_')
            topics_without_underscore.append([topic, split_topic[1]] if len(split_topic) == 2 else [topic, topic])
    return data, topics_without_underscore

# -------------- Insertion --------------
def insert_data_to_docs_collection(docs, topics):
    not_allow_topics = ['Unnamed: 0', 'doc', 'real_topic']
    list_to_insert = []
    scores = {}
    for index, row in docs.iterrows():
        for topic in topics:
            if topic not in not_allow_topics:
                scores[topic[1]] = row[topic[0]]
        object_doc = {'index': row['Unnamed: 0'], 'doc': row['doc'], 'real_topic': row['real_topic'], 'scores': scores.copy()}
        list_to_insert.append(object_doc)
        scores.clear()
    return list_to_insert

def insert_data_to_topics_collection(topics):
    not_allow_topics = ['Unnamed: 0', 'doc', 'real_topic']
    topics_dict = {}
    for index, topic in enumerate(topics):
        if topic in not_allow_topics:
            continue
        object_score = {'index': index - 3, 'description': topic[1], 'positive': 0, 'negative': 0}
        topics_dict[topic[1]] = object_score
    return topics_dict

# -------------- Calculations --------------

def calculate_topics_score(docs_collection, topics_collection):
    for index, (topic_name, topic_data) in enumerate(topics_collection.items()):
        positive, positive_length, negative, negative_length, positive_score, negative_score = 0, 0, 0, 0, 0, 0
        for doc in docs_collection:
            topic_doc_score = doc['scores'][topic_name]
            if doc['real_topic'] == topic_name:
                positive += topic_doc_score
                positive_length += 1
            elif topic_doc_score != 0:
                negative += topic_doc_score
                negative_length += 1
        positive_score = positive/positive_length
        negative_score = negative/negative_length
        topics_collection[topic_name]['positive'] = positive_score
        topics_collection[topic_name]['negative'] = negative_score
    return topics_collection

def calculate_topics_cohesion_ratio(topics_with_scores):
    # Main logic for Cohesion Formula aggregation
    positive, negative, total_score = 0, 0, 0
    for index, (topic_name, topic_data) in enumerate(topics_with_scores.items()):
      ratio = topic_data['positive']/topic_data['negative']
      if ratio>=1:
        topic_total_score = 1 - 1/(ratio+1)
      else:
        topic_total_score = ratio / (1+ratio)
      total_score += topic_total_score
    return total_score/len(topics_with_scores)

"""##Main calculations"""

def calculate_nmi_score(ground_truth_label, new_division_labels):
    score = normalized_mutual_info_score(ground_truth_label, new_division_labels)
    return score

def calculate_coherence_score(docs_list, topics_names):
    texts = [tokenizer(t) for t in docs_list]
    word2id = Dictionary(texts)
    coherence_model = CoherenceModel(topics=topics_names,
                                     texts=texts,
                                     dictionary=word2id,
                                     coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

def calculate_cohesion_score(ground_truth_docs_list, division_labels, topics_names):
    # prepare pickles files
    path_to_save =        '/content/drive/Shareddrives/Mezada/Cohesion/Datasets/exports/mnli/doc_topic_mnli_results'
    topic_id_content =    '/content/drive/Shareddrives/Mezada/Cohesion/Datasets/exports/topics_map/topic_id_content.pkl'
    topic_id_value_map =  '/content/drive/Shareddrives/Mezada/Cohesion/Datasets/exports/topics_map/topic_id_topic_value_map.pkl'

    clusters_docs = docs_to_groups(ground_truth_docs_list, division_labels)
    create_input_to_mnli_topics_content(clusters_docs, topic_id_content)
    create_topic_id_topic_value(sorted(clusters_docs.keys()), topics_names, topic_id_value_map)

    # create csv with mnli score
    zero_shot_classifier = ZeroShotClassifier(topic_id_value_map,topic_id_content, division_labels)
    zero_shot_classifier.run_classify(division_labels, path_to_save=path_to_save)
    return cohesion_calculate(path=path_to_save + '.csv')

"""## Main Pipeline"""

def run_pipeline():
    print('----------- Starting main pipeline -------------')
    correlation_dict = {}
    error_precentage = []

    divisions_dict = create_random_divisions(ground_truth_df, POC_range)
    detailed_division_print(divisions_dict)
    for i in tqdm(POC_range):
        error_precentage = str(i)+'%'
        division_labels = divisions_dict[i]
        topics, topics_synonyms, topics_glove = create_topic_names_using_tf_idf(ground_truth_docs_list, division_labels)
        models_run_over = models_to_run_over(topics, topics_synonyms, topics_glove)
        for index, topics in enumerate(models_run_over):
            print_title(index)
            if index not in total_for_print.keys():
                total_for_print[index], nmi_score_list[index], coherence_score_list[index], cohesion_score_list[index] = [],[],[],[]
            topics_names = extract_topics_labels(topics)
            detailed_topics_print(topics_names)
            
            # calculate scores
            nmi_score = calculate_nmi_score(ground_truth_label=ground_truth_label_list, new_division_labels=division_labels)
            coherence_score = calculate_coherence_score(ground_truth_docs_list, topics_names)
            cohesion_score = calculate_cohesion_score(ground_truth_docs_list=ground_truth_docs_list, division_labels=division_labels, topics_names=topics_names)
            print_results(nmi_score, coherence_score, cohesion_score)
            total_for_print[index].append([error_precentage, nmi_score, coherence_score, cohesion_score])
            nmi_score_list[index].append(nmi_score)
            coherence_score_list[index].append(coherence_score)
            cohesion_score_list[index].append(cohesion_score)
            time.sleep(1)

"""##Results"""

labels = ['coherence', 'cohesion']
def print_result_plot():
    print("*"*10, "Scores of Coherence and Cohesion against NMI in single graph", "*"*10)
    for i in nmi_score_list.keys():
      plt.plot(POC_range, nmi_score_list[i], label = "NMI - baseline", linestyle="--")
      plt.plot(POC_range, coherence_score_list[i], label = "Coherence")
      plt.plot(POC_range, cohesion_score_list[i], label = "Cohesion")
      plt.title("{}: NMI - Coherence - Cohesion - Scores".format(titles[i]))
      plt.legend()
      plt.show()

def print_pearson_corr():
    print("*"*10, "Correlations - Pearson", "*"*10)
    for i in range(len(titles)):
        division_methods = [coherence_score_list[i], cohesion_score_list[i]]
        x_simple = np.array(nmi_score_list[i])
        y_simple = [np.array(division_methods[0]),np.array(division_methods[1])]
        my_rho = [np.corrcoef(x_simple, y_simple[0]), np.corrcoef(x_simple, y_simple[1])]
        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))
        for j,label in enumerate(labels):
            ax[j].scatter(nmi_score_list[i],division_methods[j])
            ax[j].title.set_text('nmi-'+ label + ' correlation (Pearson) = ' + "{:.3f}".format(my_rho[j][0][1]))
            ax[j].set(xlabel='nmi',ylabel=label)
        fig.subplots_adjust(wspace=.3, bottom=.10)  
        plt.suptitle(titles[i], fontsize=15)   
        plt.show()
    # https://stackabuse.com/calculating-pearson-correlation-coefficient-in-python-with-numpy/

def print_spearman_corr():
    print("*"*10, "Correlations - Spearman", "*"*10)
    for i in range(len(titles)):    
        division_methods = [coherence_score_list[i], cohesion_score_list[i]]
        x_simple = np.array(nmi_score_list[i])
        y_simple = [np.array(division_methods[0]),np.array(division_methods[1])]
        my_rho = [stats.spearmanr(x_simple, y_simple[0]), stats.spearmanr(x_simple, y_simple[1])]
        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))
        for j,label in enumerate(labels):
            ax[j].scatter(nmi_score_list[i],division_methods[j])
            ax[j].title.set_text('nmi-'+ label + ' correlation (Spearman) = ' + "{:.3f}".format(my_rho[j][0]))
            ax[j].set(xlabel='nmi',ylabel=label)
        fig.subplots_adjust(wspace=.3, bottom=.10)  
        plt.suptitle(titles[i], fontsize=15)   
        plt.show()
# print_spearman_corr()

"""##Regular - POC comparison"""

total_for_print, nmi_score_list, coherence_score_list, cohesion_score_list = {}, {}, {}, {}
titles = get_titles()
run_pipeline()

summary_print()

print_result_plot()

print_pearson_corr()

print_spearman_corr()

"""##Regular - GloVe - WordNet comparison"""

if chosen_data=="Events2012":
  topics_range = range(100,200,7)
  docs_precent = 0.4 # % of "selected_docs" to test against "selected topics"
  topics_precent = 0.3 # % of "selected topics" to test against "selected_docs"
  POC_range = range(0,101,10)
  tf_idf_common_threshold = 0.4
#selected here means - selected by configuration.
# filter df by selected topics
ground_truth_df = original_df[original_df['label'].apply(lambda label: label in topics_range)] # df of filtered

# selected documents as one list
ground_truth_docs_list = ground_truth_df['text'].tolist()

# selected labels as one list
ground_truth_label_list = ground_truth_df['label'].tolist()

total_for_print, nmi_score_list, coherence_score_list, cohesion_score_list = {}, {}, {}, {}
titles = get_titles()
run_pipeline()

summary_print()

print_result_plot()

print_pearson_corr()

